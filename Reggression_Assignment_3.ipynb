{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is Ridge Regression, and How Does It Differ from Ordinary Least Squares (OLS) Regression?\n",
        "# Ridge Regression is a form of regularized linear regression that adds a penalty to the loss function\n",
        "# proportional to the square of the magnitude of the coefficients (L2 regularization). The objective is to minimize:\n",
        "# Loss function = RSS (Residual Sum of Squares) + λ * (sum of squares of coefficients)\n",
        "# Where λ is the regularization parameter.\n",
        "# OLS regression, on the other hand, minimizes only the RSS without any penalty term.\n",
        "# Ridge Regression helps prevent overfitting by reducing the impact of large coefficients.\n",
        "\n",
        "# Example of Ridge Regression\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a sample dataset\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "# Fit a Ridge model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X, y)\n",
        "print(f\"Ridge Coefficients: {ridge_model.coef_}\")\n",
        "\n",
        "# Q2. What are the assumptions of Ridge Regression?\n",
        "# The assumptions of Ridge Regression are similar to those of OLS regression with the added feature of regularization:\n",
        "# 1. Linearity: The relationship between the independent and dependent variables is linear.\n",
        "# 2. Independence of errors: The residuals (errors) should be independent of each other.\n",
        "# 3. Homoscedasticity: Constant variance of residuals (errors).\n",
        "# 4. No perfect multicollinearity: The independent variables should not be highly correlated with each other, although Ridge can help handle high correlation.\n",
        "# 5. Normally distributed residuals: While this assumption is not mandatory for Ridge, it is typically assumed for inference.\n",
        "\n",
        "# Q3. How Do You Select the Value of the Tuning Parameter (λ) in Ridge Regression?\n",
        "# The tuning parameter λ (also called alpha) controls the strength of the regularization.\n",
        "# A larger λ results in more regularization (shrinking coefficients more), and a smaller λ results in less regularization.\n",
        "# The optimal value of λ is often selected using cross-validation. Grid search or random search are commonly used for this purpose.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Grid search to find the best alpha value\n",
        "parameters = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
        "ridge_grid = GridSearchCV(Ridge(), parameters, cv=5)\n",
        "ridge_grid.fit(X, y)\n",
        "print(f\"Best alpha value: {ridge_grid.best_params_['alpha']}\")\n",
        "\n",
        "# Q4. Can Ridge Regression be Used for Feature Selection? If Yes, How?\n",
        "# Ridge Regression does not perform feature selection in the same way that Lasso regression does.\n",
        "# Lasso (L1 regularization) can shrink some coefficients to zero, effectively removing them from the model.\n",
        "# Ridge, on the other hand, only shrinks coefficients towards zero without completely removing them.\n",
        "# Thus, while Ridge helps with regularization, it does not provide a direct method for feature selection.\n",
        "# However, it can still help to improve the model by reducing the influence of irrelevant features.\n",
        "\n",
        "# Q5. How Does the Ridge Regression Model Perform in the Presence of Multicollinearity?\n",
        "# Ridge Regression performs well in the presence of multicollinearity (when independent variables are highly correlated).\n",
        "# In ordinary least squares (OLS) regression, multicollinearity can lead to large variances for coefficient estimates,\n",
        "# making the model sensitive to small changes in data.\n",
        "# Ridge addresses this by adding a penalty to the size of coefficients, which stabilizes the estimates even in the presence of multicollinearity.\n",
        "\n",
        "# Q6. Can Ridge Regression Handle Both Categorical and Continuous Independent Variables?\n",
        "# Ridge Regression can handle both categorical and continuous independent variables, but the categorical variables must be properly encoded.\n",
        "# Categorical variables are typically encoded using techniques like one-hot encoding or label encoding before being passed into the model.\n",
        "# Continuous variables are directly used in the regression model as they are.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Example: Handling categorical data using one-hot encoding\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Example data with categorical variable\n",
        "X_categorical = np.array([['A'], ['B'], ['C'], ['A'], ['B']])\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "# One-hot encoding for the categorical variable\n",
        "encoder = ColumnTransformer(\n",
        "    transformers=[('cat', OneHotEncoder(), [0])],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Ridge model with preprocessing\n",
        "ridge_with_encoding = Pipeline(steps=[('preprocessor', encoder),\n",
        "                                      ('regressor', Ridge(alpha=1))])\n",
        "\n",
        "ridge_with_encoding.fit(X_categorical, y)\n",
        "print(f\"Ridge Coefficients: {ridge_with_encoding.named_steps['regressor'].coef_}\")\n",
        "\n",
        "# Q7. How Do You Interpret the Coefficients of Ridge Regression?\n",
        "# The coefficients of Ridge Regression are interpreted in the same way as those of OLS regression:\n",
        "# - A positive coefficient means that as the corresponding independent variable increases, the dependent variable increases.\n",
        "# - A negative coefficient means that as the independent variable increases, the dependent variable decreases.\n",
        "# - The magnitude of the coefficient indicates the strength of the relationship between the independent and dependent variables.\n",
        "# Ridge regression coefficients are generally smaller due to the penalty imposed by regularization, especially when λ is large.\n",
        "\n",
        "# Q8. Can Ridge Regression Be Used for Time-Series Data Analysis? If Yes, How?\n",
        "# Yes, Ridge Regression can be used for time-series data analysis, but with some considerations:\n",
        "# - Time-series data often have autocorrelation (dependency between observations), which needs to be handled.\n",
        "# - Ridge can be used in time-series forecasting models, especially when there are multiple predictors (lag features, external variables).\n",
        "# - It's important to account for time dependencies using techniques like lag features, differencing, or using time-series specific models (e.g., ARIMA).\n",
        "# Ridge Regression can work with lagged values or rolling window features that represent past data points to predict future values.\n",
        "\n",
        "# Example: Using Ridge Regression for Time-Series Forecasting (simplified)\n",
        "import pandas as pd\n",
        "\n",
        "# Create a simple time-series dataset\n",
        "data = pd.DataFrame({\n",
        "    't': [1, 2, 3, 4, 5],\n",
        "    'y': [1, 2, 3, 4, 5]\n",
        "})\n",
        "\n",
        "# Use previous value (lag) as feature for Ridge\n",
        "data['y_lag'] = data['y'].shift(1)\n",
        "\n",
        "# Drop the first row (because of NaN in the lag column)\n",
        "data = data.dropna()\n",
        "\n",
        "X_ts = data[['y_lag']]\n",
        "y_ts = data['y']\n",
        "\n",
        "# Fit Ridge model\n",
        "ridge_ts = Ridge(alpha=1.0)\n",
        "ridge_ts.fit(X_ts, y_ts)\n",
        "print(f\"Ridge Coefficients (Time-Series): {ridge_ts.coef_}\")\n"
      ]
    }
  ]
}